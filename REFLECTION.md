# Agent Experiment Reflection

This document summarizes observations from two experimental runs of the agent:
with iterative refinement enabled (`results_tool_refine.json`) and the ablation run
(`results_toolonly.json`). The plots referenced below can be generated with
`python scripts/plot_results.py`.

## How Well the Agent Performs and Why

- **Success rate** – With refinement enabled, the agent solved all 20 tasks
  (100% success). The tool-only variant solved 16 of 20 tasks.
- **Attempts** – On average, the refinement run required about 1.45 attempts per
task while the ablation run solved each task in a single attempt or failed.
- **Refinement helps** – Iterative self-refinement is crucial for handling
  Verilator errors that require multiple code changes. Without refinement, the
  agent cannot recover from initial failures.
- **Speed vs. success** – Refinement took about 26 minutes for all tasks
  (~79 s per task) compared with 18 minutes (~53 s per task) for the tool-only
  run, so the higher success rate comes at the cost of longer runtimes.

## Why the Agent Succeeds or Fails

The successful run shows that the agent can fix typical lint violations when it
can see compiler output and try again. Failure modes arise mainly from
multi-step fixes or complex Verilog constructs that exceed the context or are
underrepresented in the base model's training data.

## Scaling Up Inference

Larger models or those with extended context windows could better understand
hardware design patterns and long build logs. Increasing the tool-call budget
allows more iterative steps, but at a higher cost. Running inference on GPUs and
using batching can hide latency when evaluating many tasks.

## Training a Better Base Model

Pretrain or continue training on public HDL repositories and synthesis logs to
improve the model's familiarity with Verilog idioms. Domain-adaptive pretraining
on large corpora of code and build errors can yield significant gains before any
supervised finetuning.

## Collecting Training Data

Look for open-source projects (OpenROAD, OpenTitan, chipyard, etc.) that have
rich histories of bug fixes. Mining diffs where lint or synthesis failures were
addressed provides paired examples of errors and fixes. Cleaning involves
removing proprietary information and normalizing formatting. Synthetic data can
be generated by intentionally corrupting designs and recording the fixes.

## Involving Human Annotators

With a team of design and verification engineers, create a review loop where the
agent proposes fixes and humans label whether the patch compiles or needs work.
Annotators can also write additional test benches or isolate minimal examples of
bugs. Their feedback expands the dataset with high-quality demonstrations and
reduces noise in automatically mined diffs.